{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL (NATURAL LANGUAGE PROCESSING)\n",
    " - NLP is a field of linguistics and machine learning focused on understanding everything related to human language.\n",
    " - The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words.\n",
    " - The following is a list of common NLP tasks, with some examples of each:\n",
    "\n",
    "1. `Classifying whole sentences`: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not\n",
    "\n",
    "2. `Classifying each word in a sentence`: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization)\n",
    "\n",
    "3. `Generating text content`: Completing a prompt with auto-generated text, filling in the blanks in a text with masked words\n",
    "\n",
    "4. `Extracting an answer from a text`: Given a question and a context, extracting the answer to the question based on the information provided in the context\n",
    "\n",
    "5. `Generating a new sentence from an input text`: Translating a text into another language, summarizing a text\n",
    "NLP isn’t limited to written text though. It also tackles complex challenges in speech recognition and computer vision, such as generating a transcript of an audio sample or a description of an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "\n",
    "classifier(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This course is about DINOSEURS',\n",
       " 'labels': ['history', 'education', 'science'],\n",
       " 'scores': [0.5254783034324646, 0.3027774691581726, 0.17174425721168518]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero shot clssification\n",
    "# Helps us to clssify text that have not been labelled\n",
    "zero_shot_classifer = pipeline(\"zero-shot-classification\")\n",
    "zero_shot_classifer(\"This course is about DINOSEURS\",  candidate_labels =[\"education\", \"science\", \"history\"])\n",
    "\n",
    "#This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it.\n",
    "# It can directly return probability scores for any list of labels you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I would like to thank Mr. Fassbender and his staff for their hard work. Please also let me know if your restaurant ever loses customers. I would also like to thank Mr. Pankaja and his staff for helping with my food request'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"hello Boys, how's your record going? I\"}]\n",
      "[{'generated_text': 'hello Boys, how does he know you\\'ve been playing his fantasy football game this season?\"\\n\\n\"We do know this.\"\\n\\nThat comment, for many fans, didn\\'t quite make it inside the room until the last hour of the game'}, {'generated_text': 'hello Boys, how we got together and started on her family, we\\'ll definitely be back next year.\\n\\n\"We\\'re all very happy. We\\'re so well. We\\'re all trying to pull it together.\"'}, {'generated_text': 'hello Boys, how much do you love to play hockey? This year there\\'s an interesting team called \"Guns of Wobblie\" that is a bunch of small-town kids who play as \\'Boys.\\' They play on the main'}, {'generated_text': \"hello Boys, how are we holding on? The best thing to say is that as soon as there's a change in perspective, there's not necessarily a huge problem, especially if you're young. You need to make a conscious effort to have any\"}, {'generated_text': 'hello Boys, how can this be?\\n\\nWhy did it get me? So often I got caught up in a culture of failure, where it was like that you just know a little bit better, just put something else in your body and get'}]\n"
     ]
    }
   ],
   "source": [
    "# TEXT GENERATION\n",
    "# this is similar to the predictive text feature that is found on many phones\n",
    "# You can control how many different sequences are generated with the argument num_return_sequences \n",
    "# and the total length of the output text with the argument max_length.\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "\n",
    "print(text_generator(\"I would like to thank Mr.\"))\n",
    "\n",
    "print(text_generator(\"hello Boys, how\", max_length=10)) # maybe use with apdding=True\n",
    "\n",
    "print(text_generator(\"hello Boys, how\", num_return_sequences=5)) # return array of 5 outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello Boss, how can i tell about it? It's a puzzle that you've always played in a computer game, but then you're all here, you can go. It's really a game of playing and solving puzzles. It looks like your\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using specific model from the transformers Hub using pipeline function\n",
    "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "text_generator(\"Hello Boss, how can i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.17569412291049957,\n",
       "  'token': 182,\n",
       "  'token_str': ' very',\n",
       "  'sequence': 'hello buddy , you are very nice today'},\n",
       " {'score': 0.1327628642320633,\n",
       "  'token': 546,\n",
       "  'token_str': ' looking',\n",
       "  'sequence': 'hello buddy , you are looking nice today'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill MASK\n",
    "unmask_it = pipeline(\"fill-mask\")\n",
    "unmask_it(\"hello buddy , you are <mask> nice today\", top_k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9989223,\n",
       "  'word': 'Sachin',\n",
       "  'start': 11,\n",
       "  'end': 17},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9930562,\n",
       "  'word': 'Axomiym Labs',\n",
       "  'start': 32,\n",
       "  'end': 44},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9909345,\n",
       "  'word': 'Kochi',\n",
       "  'start': 48,\n",
       "  'end': 53}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named Entity recognition (NER)\n",
    "# catagories based on persons,location,orgainization\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sachin and I work at Axomiym Labs in Kochi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9574574828147888, 'start': 31, 'end': 43, 'answer': 'Axomium albs'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question Answering\n",
    "# The question-answering pipeline answers questions using information from a given context:\n",
    "\n",
    "qr = pipeline(\"question-answering\")\n",
    "qr(question=\"Where do i work\", context=\"My name is Sachin and works at Axomium albs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . There are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues . There is greater concentration on high-tech subjects, largely supporting increasingly complex scientific developments . While the latter is important, it should not be at the expense of more traditional engineering .'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarization\n",
    "summar = pipeline(\"summarization\")\n",
    "summar(\"\"\" America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
